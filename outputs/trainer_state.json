{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.948453608247423,
  "eval_steps": 500,
  "global_step": 120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.041237113402061855,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 4.3097,
      "step": 1
    },
    {
      "epoch": 0.08247422680412371,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 4.5952,
      "step": 2
    },
    {
      "epoch": 0.12371134020618557,
      "grad_norm": 7.975068092346191,
      "learning_rate": 0.0001,
      "loss": 4.3565,
      "step": 3
    },
    {
      "epoch": 0.16494845360824742,
      "grad_norm": 8.082341194152832,
      "learning_rate": 0.0002,
      "loss": 4.2318,
      "step": 4
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 5.767001152038574,
      "learning_rate": 0.00019830508474576273,
      "loss": 4.0527,
      "step": 5
    },
    {
      "epoch": 0.24742268041237114,
      "grad_norm": NaN,
      "learning_rate": 0.00019830508474576273,
      "loss": 2.5596,
      "step": 6
    },
    {
      "epoch": 0.28865979381443296,
      "grad_norm": 9.273921012878418,
      "learning_rate": 0.00019661016949152545,
      "loss": 2.7925,
      "step": 7
    },
    {
      "epoch": 0.32989690721649484,
      "grad_norm": 11.418275833129883,
      "learning_rate": 0.00019491525423728814,
      "loss": 1.8995,
      "step": 8
    },
    {
      "epoch": 0.3711340206185567,
      "grad_norm": 5.420872688293457,
      "learning_rate": 0.00019322033898305085,
      "loss": 1.2335,
      "step": 9
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 19.2790470123291,
      "learning_rate": 0.00019152542372881357,
      "loss": 1.2317,
      "step": 10
    },
    {
      "epoch": 0.4536082474226804,
      "grad_norm": 18.002668380737305,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.1472,
      "step": 11
    },
    {
      "epoch": 0.4948453608247423,
      "grad_norm": 16.461990356445312,
      "learning_rate": 0.000188135593220339,
      "loss": 1.0707,
      "step": 12
    },
    {
      "epoch": 0.5360824742268041,
      "grad_norm": 11.065617561340332,
      "learning_rate": 0.0001864406779661017,
      "loss": 0.9883,
      "step": 13
    },
    {
      "epoch": 0.5773195876288659,
      "grad_norm": 10.9364595413208,
      "learning_rate": 0.00018474576271186442,
      "loss": 0.8807,
      "step": 14
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 13.721619606018066,
      "learning_rate": 0.00018305084745762714,
      "loss": 0.8392,
      "step": 15
    },
    {
      "epoch": 0.6597938144329897,
      "grad_norm": 11.678194046020508,
      "learning_rate": 0.00018135593220338985,
      "loss": 0.8625,
      "step": 16
    },
    {
      "epoch": 0.7010309278350515,
      "grad_norm": 3.828727960586548,
      "learning_rate": 0.00017966101694915257,
      "loss": 0.7246,
      "step": 17
    },
    {
      "epoch": 0.7422680412371134,
      "grad_norm": 2.633709192276001,
      "learning_rate": 0.00017796610169491526,
      "loss": 0.7547,
      "step": 18
    },
    {
      "epoch": 0.7835051546391752,
      "grad_norm": 1.81498384475708,
      "learning_rate": 0.00017627118644067798,
      "loss": 0.7383,
      "step": 19
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 8.54581356048584,
      "learning_rate": 0.0001745762711864407,
      "loss": 0.7423,
      "step": 20
    },
    {
      "epoch": 0.865979381443299,
      "grad_norm": 1.8073853254318237,
      "learning_rate": 0.00017288135593220342,
      "loss": 0.7306,
      "step": 21
    },
    {
      "epoch": 0.9072164948453608,
      "grad_norm": 1.7725886106491089,
      "learning_rate": 0.0001711864406779661,
      "loss": 0.6835,
      "step": 22
    },
    {
      "epoch": 0.9484536082474226,
      "grad_norm": 4.925337314605713,
      "learning_rate": 0.00016949152542372882,
      "loss": 0.6589,
      "step": 23
    },
    {
      "epoch": 0.9896907216494846,
      "grad_norm": 4.574302673339844,
      "learning_rate": 0.00016779661016949154,
      "loss": 0.7428,
      "step": 24
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 1.5696537494659424,
      "learning_rate": 0.00016610169491525423,
      "loss": 0.6874,
      "step": 25
    },
    {
      "epoch": 1.0721649484536082,
      "grad_norm": 2.913731575012207,
      "learning_rate": 0.00016440677966101695,
      "loss": 0.6599,
      "step": 26
    },
    {
      "epoch": 1.1134020618556701,
      "grad_norm": 1.3465676307678223,
      "learning_rate": 0.00016271186440677967,
      "loss": 0.7031,
      "step": 27
    },
    {
      "epoch": 1.1546391752577319,
      "grad_norm": 2.3136863708496094,
      "learning_rate": 0.00016101694915254236,
      "loss": 0.6896,
      "step": 28
    },
    {
      "epoch": 1.1958762886597938,
      "grad_norm": 1.5077693462371826,
      "learning_rate": 0.00015932203389830508,
      "loss": 0.6658,
      "step": 29
    },
    {
      "epoch": 1.2371134020618557,
      "grad_norm": 1.2724553346633911,
      "learning_rate": 0.0001576271186440678,
      "loss": 0.7084,
      "step": 30
    },
    {
      "epoch": 1.2783505154639174,
      "grad_norm": 1.0831151008605957,
      "learning_rate": 0.00015593220338983051,
      "loss": 0.6424,
      "step": 31
    },
    {
      "epoch": 1.3195876288659794,
      "grad_norm": 0.8874591588973999,
      "learning_rate": 0.00015423728813559323,
      "loss": 0.6573,
      "step": 32
    },
    {
      "epoch": 1.3608247422680413,
      "grad_norm": 3.7310848236083984,
      "learning_rate": 0.00015254237288135592,
      "loss": 0.6658,
      "step": 33
    },
    {
      "epoch": 1.402061855670103,
      "grad_norm": 1.5725816488265991,
      "learning_rate": 0.00015084745762711864,
      "loss": 0.658,
      "step": 34
    },
    {
      "epoch": 1.443298969072165,
      "grad_norm": 1.567758560180664,
      "learning_rate": 0.00014915254237288136,
      "loss": 0.653,
      "step": 35
    },
    {
      "epoch": 1.4845360824742269,
      "grad_norm": 0.6504025459289551,
      "learning_rate": 0.00014745762711864408,
      "loss": 0.64,
      "step": 36
    },
    {
      "epoch": 1.5257731958762886,
      "grad_norm": 0.7469307780265808,
      "learning_rate": 0.00014576271186440677,
      "loss": 0.6079,
      "step": 37
    },
    {
      "epoch": 1.5670103092783505,
      "grad_norm": 0.4876761734485626,
      "learning_rate": 0.00014406779661016949,
      "loss": 0.6091,
      "step": 38
    },
    {
      "epoch": 1.6082474226804124,
      "grad_norm": 1.7762919664382935,
      "learning_rate": 0.0001423728813559322,
      "loss": 0.6353,
      "step": 39
    },
    {
      "epoch": 1.6494845360824741,
      "grad_norm": 2.326618194580078,
      "learning_rate": 0.00014067796610169492,
      "loss": 0.6033,
      "step": 40
    },
    {
      "epoch": 1.690721649484536,
      "grad_norm": 0.6474783420562744,
      "learning_rate": 0.00013898305084745764,
      "loss": 0.5877,
      "step": 41
    },
    {
      "epoch": 1.731958762886598,
      "grad_norm": 1.5834907293319702,
      "learning_rate": 0.00013728813559322033,
      "loss": 0.5532,
      "step": 42
    },
    {
      "epoch": 1.7731958762886597,
      "grad_norm": 0.7576637268066406,
      "learning_rate": 0.00013559322033898305,
      "loss": 0.6348,
      "step": 43
    },
    {
      "epoch": 1.8144329896907216,
      "grad_norm": 1.7822202444076538,
      "learning_rate": 0.00013389830508474577,
      "loss": 0.6282,
      "step": 44
    },
    {
      "epoch": 1.8556701030927836,
      "grad_norm": 1.2053619623184204,
      "learning_rate": 0.00013220338983050849,
      "loss": 0.6163,
      "step": 45
    },
    {
      "epoch": 1.8969072164948453,
      "grad_norm": 0.8473104238510132,
      "learning_rate": 0.0001305084745762712,
      "loss": 0.566,
      "step": 46
    },
    {
      "epoch": 1.9381443298969072,
      "grad_norm": 1.1422940492630005,
      "learning_rate": 0.0001288135593220339,
      "loss": 0.5943,
      "step": 47
    },
    {
      "epoch": 1.9793814432989691,
      "grad_norm": 1.1317132711410522,
      "learning_rate": 0.0001271186440677966,
      "loss": 0.6002,
      "step": 48
    },
    {
      "epoch": 2.020618556701031,
      "grad_norm": 1.3247075080871582,
      "learning_rate": 0.00012542372881355933,
      "loss": 0.6492,
      "step": 49
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 0.5202934741973877,
      "learning_rate": 0.00012372881355932205,
      "loss": 0.6171,
      "step": 50
    },
    {
      "epoch": 2.1030927835051547,
      "grad_norm": 0.9968394637107849,
      "learning_rate": 0.00012203389830508477,
      "loss": 0.558,
      "step": 51
    },
    {
      "epoch": 2.1443298969072164,
      "grad_norm": 0.7434424757957458,
      "learning_rate": 0.00012033898305084746,
      "loss": 0.5806,
      "step": 52
    },
    {
      "epoch": 2.1855670103092786,
      "grad_norm": 1.873857855796814,
      "learning_rate": 0.00011864406779661017,
      "loss": 0.6124,
      "step": 53
    },
    {
      "epoch": 2.2268041237113403,
      "grad_norm": 1.526200532913208,
      "learning_rate": 0.00011694915254237289,
      "loss": 0.5957,
      "step": 54
    },
    {
      "epoch": 2.268041237113402,
      "grad_norm": 0.6575198173522949,
      "learning_rate": 0.0001152542372881356,
      "loss": 0.5788,
      "step": 55
    },
    {
      "epoch": 2.3092783505154637,
      "grad_norm": 1.2470817565917969,
      "learning_rate": 0.00011355932203389832,
      "loss": 0.6129,
      "step": 56
    },
    {
      "epoch": 2.350515463917526,
      "grad_norm": 1.4979007244110107,
      "learning_rate": 0.00011186440677966102,
      "loss": 0.5609,
      "step": 57
    },
    {
      "epoch": 2.3917525773195876,
      "grad_norm": 1.0009788274765015,
      "learning_rate": 0.00011016949152542372,
      "loss": 0.5136,
      "step": 58
    },
    {
      "epoch": 2.4329896907216497,
      "grad_norm": 1.1986441612243652,
      "learning_rate": 0.00010847457627118644,
      "loss": 0.5271,
      "step": 59
    },
    {
      "epoch": 2.4742268041237114,
      "grad_norm": 1.9208089113235474,
      "learning_rate": 0.00010677966101694916,
      "loss": 0.5652,
      "step": 60
    },
    {
      "epoch": 2.515463917525773,
      "grad_norm": 1.1244500875473022,
      "learning_rate": 0.00010508474576271188,
      "loss": 0.5392,
      "step": 61
    },
    {
      "epoch": 2.556701030927835,
      "grad_norm": 1.125799536705017,
      "learning_rate": 0.00010338983050847457,
      "loss": 0.5293,
      "step": 62
    },
    {
      "epoch": 2.597938144329897,
      "grad_norm": 1.6051812171936035,
      "learning_rate": 0.00010169491525423729,
      "loss": 0.5481,
      "step": 63
    },
    {
      "epoch": 2.6391752577319587,
      "grad_norm": 1.1794084310531616,
      "learning_rate": 0.0001,
      "loss": 0.5595,
      "step": 64
    },
    {
      "epoch": 2.680412371134021,
      "grad_norm": 1.2302409410476685,
      "learning_rate": 9.830508474576272e-05,
      "loss": 0.5751,
      "step": 65
    },
    {
      "epoch": 2.7216494845360826,
      "grad_norm": 0.7583317756652832,
      "learning_rate": 9.661016949152543e-05,
      "loss": 0.5297,
      "step": 66
    },
    {
      "epoch": 2.7628865979381443,
      "grad_norm": 1.4804943799972534,
      "learning_rate": 9.491525423728815e-05,
      "loss": 0.5814,
      "step": 67
    },
    {
      "epoch": 2.804123711340206,
      "grad_norm": 1.085852026939392,
      "learning_rate": 9.322033898305085e-05,
      "loss": 0.5319,
      "step": 68
    },
    {
      "epoch": 2.845360824742268,
      "grad_norm": 0.6378822326660156,
      "learning_rate": 9.152542372881357e-05,
      "loss": 0.5043,
      "step": 69
    },
    {
      "epoch": 2.88659793814433,
      "grad_norm": 0.8643338680267334,
      "learning_rate": 8.983050847457629e-05,
      "loss": 0.5527,
      "step": 70
    },
    {
      "epoch": 2.927835051546392,
      "grad_norm": 0.8674790859222412,
      "learning_rate": 8.813559322033899e-05,
      "loss": 0.5882,
      "step": 71
    },
    {
      "epoch": 2.9690721649484537,
      "grad_norm": 1.5106830596923828,
      "learning_rate": 8.644067796610171e-05,
      "loss": 0.5357,
      "step": 72
    },
    {
      "epoch": 3.0103092783505154,
      "grad_norm": 1.0752545595169067,
      "learning_rate": 8.474576271186441e-05,
      "loss": 0.6317,
      "step": 73
    },
    {
      "epoch": 3.051546391752577,
      "grad_norm": 0.9981132745742798,
      "learning_rate": 8.305084745762712e-05,
      "loss": 0.5034,
      "step": 74
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 0.9705207347869873,
      "learning_rate": 8.135593220338983e-05,
      "loss": 0.6295,
      "step": 75
    },
    {
      "epoch": 3.134020618556701,
      "grad_norm": 0.6499972939491272,
      "learning_rate": 7.966101694915254e-05,
      "loss": 0.4733,
      "step": 76
    },
    {
      "epoch": 3.1752577319587627,
      "grad_norm": 0.591846764087677,
      "learning_rate": 7.796610169491526e-05,
      "loss": 0.5043,
      "step": 77
    },
    {
      "epoch": 3.216494845360825,
      "grad_norm": 0.5526354312896729,
      "learning_rate": 7.627118644067796e-05,
      "loss": 0.4589,
      "step": 78
    },
    {
      "epoch": 3.2577319587628866,
      "grad_norm": 1.389411449432373,
      "learning_rate": 7.457627118644068e-05,
      "loss": 0.4986,
      "step": 79
    },
    {
      "epoch": 3.2989690721649483,
      "grad_norm": 1.6703987121582031,
      "learning_rate": 7.288135593220338e-05,
      "loss": 0.5142,
      "step": 80
    },
    {
      "epoch": 3.3402061855670104,
      "grad_norm": 0.6960219740867615,
      "learning_rate": 7.11864406779661e-05,
      "loss": 0.4701,
      "step": 81
    },
    {
      "epoch": 3.381443298969072,
      "grad_norm": 0.8940098285675049,
      "learning_rate": 6.949152542372882e-05,
      "loss": 0.5103,
      "step": 82
    },
    {
      "epoch": 3.422680412371134,
      "grad_norm": 1.1182429790496826,
      "learning_rate": 6.779661016949152e-05,
      "loss": 0.4572,
      "step": 83
    },
    {
      "epoch": 3.463917525773196,
      "grad_norm": 1.4408694505691528,
      "learning_rate": 6.610169491525424e-05,
      "loss": 0.459,
      "step": 84
    },
    {
      "epoch": 3.5051546391752577,
      "grad_norm": 1.6515578031539917,
      "learning_rate": 6.440677966101695e-05,
      "loss": 0.4695,
      "step": 85
    },
    {
      "epoch": 3.5463917525773194,
      "grad_norm": 1.7353509664535522,
      "learning_rate": 6.271186440677966e-05,
      "loss": 0.5077,
      "step": 86
    },
    {
      "epoch": 3.5876288659793816,
      "grad_norm": 1.0459580421447754,
      "learning_rate": 6.101694915254238e-05,
      "loss": 0.4739,
      "step": 87
    },
    {
      "epoch": 3.6288659793814433,
      "grad_norm": 0.9440127611160278,
      "learning_rate": 5.932203389830509e-05,
      "loss": 0.4924,
      "step": 88
    },
    {
      "epoch": 3.670103092783505,
      "grad_norm": 1.6688951253890991,
      "learning_rate": 5.76271186440678e-05,
      "loss": 0.4833,
      "step": 89
    },
    {
      "epoch": 3.711340206185567,
      "grad_norm": 0.8871834874153137,
      "learning_rate": 5.593220338983051e-05,
      "loss": 0.4866,
      "step": 90
    },
    {
      "epoch": 3.752577319587629,
      "grad_norm": 0.7707380056381226,
      "learning_rate": 5.423728813559322e-05,
      "loss": 0.4532,
      "step": 91
    },
    {
      "epoch": 3.7938144329896906,
      "grad_norm": 0.9277686476707458,
      "learning_rate": 5.254237288135594e-05,
      "loss": 0.4425,
      "step": 92
    },
    {
      "epoch": 3.8350515463917527,
      "grad_norm": 0.765130341053009,
      "learning_rate": 5.0847457627118643e-05,
      "loss": 0.4136,
      "step": 93
    },
    {
      "epoch": 3.8762886597938144,
      "grad_norm": 1.0933336019515991,
      "learning_rate": 4.915254237288136e-05,
      "loss": 0.4508,
      "step": 94
    },
    {
      "epoch": 3.917525773195876,
      "grad_norm": 0.5974969863891602,
      "learning_rate": 4.745762711864407e-05,
      "loss": 0.4203,
      "step": 95
    },
    {
      "epoch": 3.9587628865979383,
      "grad_norm": 1.0910917520523071,
      "learning_rate": 4.5762711864406784e-05,
      "loss": 0.4968,
      "step": 96
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9217063784599304,
      "learning_rate": 4.4067796610169495e-05,
      "loss": 0.4503,
      "step": 97
    },
    {
      "epoch": 4.041237113402062,
      "grad_norm": 0.6293262243270874,
      "learning_rate": 4.2372881355932206e-05,
      "loss": 0.38,
      "step": 98
    },
    {
      "epoch": 4.082474226804123,
      "grad_norm": 0.7337431907653809,
      "learning_rate": 4.067796610169492e-05,
      "loss": 0.38,
      "step": 99
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 1.1707102060317993,
      "learning_rate": 3.898305084745763e-05,
      "loss": 0.4173,
      "step": 100
    },
    {
      "epoch": 4.164948453608248,
      "grad_norm": 0.7680075764656067,
      "learning_rate": 3.728813559322034e-05,
      "loss": 0.4539,
      "step": 101
    },
    {
      "epoch": 4.206185567010309,
      "grad_norm": 0.9018271565437317,
      "learning_rate": 3.559322033898305e-05,
      "loss": 0.435,
      "step": 102
    },
    {
      "epoch": 4.247422680412371,
      "grad_norm": 1.258994460105896,
      "learning_rate": 3.389830508474576e-05,
      "loss": 0.4112,
      "step": 103
    },
    {
      "epoch": 4.288659793814433,
      "grad_norm": 0.9512425065040588,
      "learning_rate": 3.2203389830508473e-05,
      "loss": 0.4063,
      "step": 104
    },
    {
      "epoch": 4.329896907216495,
      "grad_norm": 1.1437878608703613,
      "learning_rate": 3.050847457627119e-05,
      "loss": 0.4044,
      "step": 105
    },
    {
      "epoch": 4.371134020618557,
      "grad_norm": 0.9906209111213684,
      "learning_rate": 2.88135593220339e-05,
      "loss": 0.4529,
      "step": 106
    },
    {
      "epoch": 4.412371134020619,
      "grad_norm": 1.1744062900543213,
      "learning_rate": 2.711864406779661e-05,
      "loss": 0.3985,
      "step": 107
    },
    {
      "epoch": 4.453608247422681,
      "grad_norm": 1.2847121953964233,
      "learning_rate": 2.5423728813559322e-05,
      "loss": 0.4136,
      "step": 108
    },
    {
      "epoch": 4.494845360824742,
      "grad_norm": 0.9597761631011963,
      "learning_rate": 2.3728813559322036e-05,
      "loss": 0.3746,
      "step": 109
    },
    {
      "epoch": 4.536082474226804,
      "grad_norm": 0.9520485401153564,
      "learning_rate": 2.2033898305084748e-05,
      "loss": 0.3851,
      "step": 110
    },
    {
      "epoch": 4.577319587628866,
      "grad_norm": 0.7259764671325684,
      "learning_rate": 2.033898305084746e-05,
      "loss": 0.3803,
      "step": 111
    },
    {
      "epoch": 4.618556701030927,
      "grad_norm": 1.0837011337280273,
      "learning_rate": 1.864406779661017e-05,
      "loss": 0.4577,
      "step": 112
    },
    {
      "epoch": 4.65979381443299,
      "grad_norm": 1.0398790836334229,
      "learning_rate": 1.694915254237288e-05,
      "loss": 0.4214,
      "step": 113
    },
    {
      "epoch": 4.701030927835052,
      "grad_norm": 1.2157477140426636,
      "learning_rate": 1.5254237288135596e-05,
      "loss": 0.4159,
      "step": 114
    },
    {
      "epoch": 4.742268041237113,
      "grad_norm": 0.8811137676239014,
      "learning_rate": 1.3559322033898305e-05,
      "loss": 0.4191,
      "step": 115
    },
    {
      "epoch": 4.783505154639175,
      "grad_norm": 1.3526266813278198,
      "learning_rate": 1.1864406779661018e-05,
      "loss": 0.3878,
      "step": 116
    },
    {
      "epoch": 4.824742268041237,
      "grad_norm": 0.8755431771278381,
      "learning_rate": 1.016949152542373e-05,
      "loss": 0.3987,
      "step": 117
    },
    {
      "epoch": 4.8659793814432994,
      "grad_norm": 0.8808130621910095,
      "learning_rate": 8.47457627118644e-06,
      "loss": 0.3899,
      "step": 118
    },
    {
      "epoch": 4.907216494845361,
      "grad_norm": 0.7695167660713196,
      "learning_rate": 6.779661016949153e-06,
      "loss": 0.4826,
      "step": 119
    },
    {
      "epoch": 4.948453608247423,
      "grad_norm": 0.837993323802948,
      "learning_rate": 5.084745762711865e-06,
      "loss": 0.437,
      "step": 120
    },
    {
      "epoch": 4.948453608247423,
      "step": 120,
      "total_flos": 7.846383411265536e+16,
      "train_loss": 0.7774592893819015,
      "train_runtime": 2304.296,
      "train_samples_per_second": 1.684,
      "train_steps_per_second": 0.052
    }
  ],
  "logging_steps": 1,
  "max_steps": 120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.846383411265536e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
